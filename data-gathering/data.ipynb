{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/GTSF-Quantitative-Sector/sec_parser.git python-dotenv tqdm pickle matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec import constants, stock, lookups, processor\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "from typing import Iterator\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import dotenv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "API_KEY = os.getenv(\"POLYGON_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants.set_polygon_key(API_KEY)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cells to get the most recent financials data in SEC parser. Should be run every month or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not stale. Skipping download. To override, set force_update=True or change max_stale_days.\n"
     ]
    }
   ],
   "source": [
    "processor.download_sec_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gather_with_concurrency(n, *coros):\n",
    "    semaphore = asyncio.Semaphore(n)\n",
    "    async def sem_coro(coro):\n",
    "        async with semaphore:\n",
    "            return await coro\n",
    "    return await asyncio.gather(*(sem_coro(c) for c in coros), return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_value(value_func):\n",
    "    try:\n",
    "        return value_func()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_data(stocks: dict, query_date: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Gather all the data needed for the embeddings for each given stock.\n",
    "\n",
    "    Args:\n",
    "        stocks (dict): Dictionary of stocks to gather data for: {ticker (str): stock (stock.Stock)}\n",
    "        query_date (str, optional): Date in YYYY-MM-DD format. Defaults to None, which gets the latest metric.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of data for each stock: {ticker (str): data (dict)}\n",
    "    \"\"\"\n",
    "\n",
    "    if query_date is None:\n",
    "        query_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # get price and rsi for each stock\n",
    "    price_coros, rsi_coros = [], []\n",
    "    for ticker, s in stocks.items():\n",
    "        price_coros.append(s.get_price(query_date))\n",
    "        rsi_coros.append(s.get_rsi(query_date))\n",
    "    results = await gather_with_concurrency(50, *price_coros, *rsi_coros)\n",
    "    price_dict = {ticker: price for ticker, price in zip(stocks.keys(), results[: len(stocks)])}\n",
    "    rsi_dict = {ticker: rsi for ticker, rsi in zip(stocks.keys(), results[len(stocks) :])}\n",
    "\n",
    "    data = {}\n",
    "    for ticker, s in stocks.items():\n",
    "        try:\n",
    "            price = price_dict[ticker]\n",
    "            shares = s.get_shares_outstanding(query_date)\n",
    "            market_cap = shares * price\n",
    "        except:\n",
    "            price = None\n",
    "            market_cap = None\n",
    "\n",
    "        # get price to book\n",
    "        try:\n",
    "            book_val = s.get_book_value(query_date)\n",
    "            bvps = book_val / shares\n",
    "            pb_ratio = price / bvps\n",
    "        except:\n",
    "            pb_ratio = None\n",
    "\n",
    "        # get price to earnings\n",
    "        try:\n",
    "            net_income = s.get_net_income(query_date)\n",
    "            try:\n",
    "                preferred_div = s.get_preferred_dividends(query_date)\n",
    "            except:\n",
    "                preferred_div = 0\n",
    "            eps = (net_income - preferred_div) / shares\n",
    "            pe_ratio = price / eps\n",
    "        except:\n",
    "            pe_ratio = None\n",
    "\n",
    "        # get price to sales\n",
    "        try:\n",
    "            revenue = s.get_revenue(query_date)\n",
    "            sps = revenue / shares\n",
    "            ps_ratio = price / sps\n",
    "        except:\n",
    "            ps_ratio = None\n",
    "\n",
    "        # get price to cash flow\n",
    "        try:\n",
    "            ocf = s.get_operating_cash_flow(query_date)\n",
    "            cfps = ocf / shares\n",
    "            pcf_ratio = price / cfps\n",
    "        except:\n",
    "            pcf_ratio = None\n",
    "\n",
    "        # get ev to ebitda\n",
    "        try:\n",
    "            debt = s.get_total_debt(query_date)\n",
    "            cash = s.get_cash(query_date)\n",
    "            ev = market_cap + debt - cash\n",
    "            ebitda = s.get_ebitda(query_date)\n",
    "            ev_ebitda = ev / ebitda\n",
    "        except:\n",
    "            ev_ebitda = None\n",
    "\n",
    "        # get shareholder yield\n",
    "        try:\n",
    "            cash_dividends = s.get_cash_dividends(query_date)\n",
    "        except:\n",
    "            cash_dividends = 0\n",
    "        try:\n",
    "            share_repurchases = s.get_share_repurchases(query_date)\n",
    "        except:\n",
    "            share_repurchases = 0\n",
    "        try:\n",
    "            share_issuances = s.get_share_issuances(query_date)\n",
    "        except:\n",
    "            share_issuances = 0\n",
    "        try:\n",
    "            debt_paydown = s.get_debt_paydown(query_date)\n",
    "        except:\n",
    "            debt_paydown = 0\n",
    "        try:\n",
    "            debt_issuance = s.get_debt_issuance(query_date)\n",
    "        except:\n",
    "            debt_issuance = 0\n",
    "\n",
    "        try:\n",
    "            shareholder_yield = (\n",
    "                cash_dividends\n",
    "                + share_repurchases\n",
    "                - share_issuances\n",
    "                + debt_paydown\n",
    "                - debt_issuance\n",
    "            ) / market_cap\n",
    "        except:\n",
    "            shareholder_yield = None\n",
    "\n",
    "        try:\n",
    "            wacc = s.get_wacc(query_date)\n",
    "        except Exception:\n",
    "            wacc = None\n",
    "            \n",
    "        rsi = rsi_dict[ticker]\n",
    "        industry = s.industry\n",
    "\n",
    "        data[ticker] = {\n",
    "            \"pb_ratio\": pb_ratio,\n",
    "            \"pe_ratio\": pe_ratio,\n",
    "            \"ps_ratio\": ps_ratio,\n",
    "            \"pcf_ratio\": pcf_ratio,\n",
    "            \"ev_ebitda\": ev_ebitda,\n",
    "            \"shareholder_yield\": shareholder_yield,\n",
    "            \"rsi\": rsi,\n",
    "            \"price\": price,\n",
    "            \"industry\": industry,\n",
    "            \"wacc\": wacc,\n",
    "        }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: dict, universe: list) -> tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Handle data corrections, removal of na values, etc..\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of data for each stock: {ticker (str): data (dict)}\n",
    "        universe (list): Set of tickers to rank (in case data contains more stocks than the universe).\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, float]: Tuple of the ranked DataFrame and the data coverage percentage.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(data).T\n",
    "    available_tickers = df.index.intersection(universe)\n",
    "    df = df.loc[available_tickers]\n",
    "\n",
    "    for column in [\"pb_ratio\", \"pe_ratio\", \"ps_ratio\", \"pcf_ratio\", \"ev_ebitda\", \"shareholder_yield\", \"rsi\", \"price\", \"wacc\"]:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    # drop rows with nan values, determine coverage percentage\n",
    "    df = df.dropna()\n",
    "    coverage = len(df) / len(universe)\n",
    "\n",
    "    # sort by rsi (high to low)\n",
    "    df = df[df[\"rsi\"].apply(lambda x: isinstance(x, float))]\n",
    "    df = df.sort_values(\"rsi\", ascending=False)\n",
    "\n",
    "    return df, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Data Collection for Current Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_for_current = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_for_current:\n",
    "    stocks = {}\n",
    "    failed = set()\n",
    "    sp500 = lookups.get_sp500_tickers()\n",
    "    for ticker in sp500:\n",
    "        try:\n",
    "            stocks[ticker] = stock.Stock(ticker)\n",
    "        except:\n",
    "            failed.add(ticker)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_for_current:\n",
    "    vc2_data = await get_data(stocks)\n",
    "    df, coverage = clean_data(vc2_data, sp500)\n",
    "    print(coverage)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date: str, end_date: str, interval: int = 1) -> Iterator[date]:\n",
    "    \"\"\"\n",
    "    Generate a range of dates between start_date and end_date.\n",
    "    \n",
    "    Args:\n",
    "        start_date (str): Start date in YYYY-MM-DD format.\n",
    "        end_date (str): End date in YYYY-MM-DD format.\n",
    "        interval (int, optional): Interval between dates in days. Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        Iterator[str]: Iterator of dates in string format between start_date and end_date.\n",
    "    \"\"\"\n",
    "\n",
    "    start_date = date.fromisoformat(start_date)\n",
    "    end_date = date.fromisoformat(end_date)\n",
    "    delta = end_date - start_date\n",
    "    for i in range(0, delta.days + 1, interval):\n",
    "        yield str(start_date + timedelta(days=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_universe(start_date: str, end_date: str, interval: int) -> set:\n",
    "    # get all tickers in the S&P 500 between start_date and end_date\n",
    "    all_sp500_tickers = set()\n",
    "    for d in daterange(start_date, end_date, interval):\n",
    "        all_sp500_tickers.update(lookups.get_sp500_tickers(d))\n",
    "\n",
    "    return all_sp500_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_get_stocks(start_date: str, end_date: str, interval: int) -> dict:\n",
    "    # initialize stocks dictionary: {ticker: stock.Stock}\n",
    "    stocks = {}\n",
    "    for ticker in get_sp500_universe(start_date, end_date, interval):\n",
    "        try:\n",
    "            processor.process_sec_json(ticker)\n",
    "            stocks[ticker] = stock.Stock(ticker)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting stock data for {ticker}: {e}\")\n",
    "\n",
    "    return stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_encoding(data: dict):\n",
    "    \"\"\"\n",
    "    Encodes industry value as a one-hot encoding in dataset.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of data for each stock at each timepoint: {date (str) : {ticker (str): data (dict)}}\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of data for each stock at each timepoint with industry one-hot encoding: {date (str) : {ticker (str): data (dict)}}\n",
    "    \"\"\"\n",
    "\n",
    "    industries = set(d[\"industry\"] for date_data in data.values() for d in date_data.values())\n",
    "    industry_dict = {industry: i for i, industry in enumerate(sorted(industries))}\n",
    "\n",
    "    for date_data in data.values():\n",
    "        for ticker, metrics in date_data.items():\n",
    "            industry_encoding = [0] * len(industries)\n",
    "            industry_encoding[industry_dict[metrics[\"industry\"]]] = 1\n",
    "            date_data[ticker][\"industry_encoded\"] = industry_encoding\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gather_historical_data(stocks: dict, start_date: str, end_date: str, interval: int) -> dict:\n",
    "    \"\"\"\n",
    "    Gather data for all stocks in the S&P 500 between start_date and end_date.\n",
    "\n",
    "    Args:\n",
    "        stocks (dict): Dictionary of stocks to gather data for: {ticker (str): stock (stock.Stock)}\n",
    "        start_date (str): Start date of the backtest.\n",
    "        end_date (str): End date of the backtest.\n",
    "        interval (int): Interval between dates in days.\n",
    "\n",
    "    Returns:\n",
    "        dict: {str(date): {ticker: {data}}}\n",
    "    \"\"\"\n",
    "    # get all data for all stocks in the S&P 500 between start_date and end_date\n",
    "    # {str(date): {ticker: {data}}\n",
    "    data = {}\n",
    "    for d in tqdm(daterange(start_date, end_date, interval)):\n",
    "        data[d] = await get_data(stocks, d)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Data Gathering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKTEST_DIR = \"saved_data\"\n",
    "\n",
    "def save_backtest_data(data: dict, start_date: str, end_date: str, interval: int, save_dir: str = BACKTEST_DIR):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(os.path.join(save_dir, f\"{start_date}_{end_date}_{interval}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GATHER_START_DATE_6M = \"2023-07-01\"\n",
    "GATHER_START_DATE_1Y = \"2023-01-01\"\n",
    "GATHER_START_DATE_2Y = \"2022-01-01\"\n",
    "GATHER_START_DATE_10Y = \"2013-01-01\"\n",
    "GATHER_END_DATE = \"2024-01-01\"\n",
    "GATHER_INTERVAL_MONTHLY, GATHER_INTERVAL_WEEKLY, GATHER_INTERVAL_DAILY = 30, 7, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_reprocess_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_reprocess_data:\n",
    "    stocks = process_and_get_stocks(GATHER_START_DATE_10Y, GATHER_END_DATE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_reprocess_data:\n",
    "    # Get monthly backtest data\n",
    "    monthly = await gather_historical_data(stocks, GATHER_START_DATE_10Y, GATHER_END_DATE, GATHER_INTERVAL_MONTHLY)\n",
    "    save_backtest_data(monthly, GATHER_START_DATE_10Y, GATHER_END_DATE, GATHER_INTERVAL_MONTHLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_reprocess_data:\n",
    "    # get weekly backtest data\n",
    "    weekly = await gather_historical_data(stocks, GATHER_START_DATE_1Y, GATHER_END_DATE, GATHER_INTERVAL_WEEKLY)\n",
    "    save_backtest_data(weekly, GATHER_START_DATE_1Y, GATHER_END_DATE, GATHER_INTERVAL_WEEKLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_reprocess_data:\n",
    "    # get daily backtest data\n",
    "    daily = await gather_historical_data(stocks, GATHER_START_DATE_6M, GATHER_END_DATE, GATHER_INTERVAL_DAILY)\n",
    "    save_backtest_data(daily, GATHER_START_DATE_6M, GATHER_END_DATE, GATHER_INTERVAL_DAILY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_reprocess_data:\n",
    "    # Get daily backtest data for 2Y\n",
    "    daily = await gather_historical_data(stocks, GATHER_START_DATE_2Y, GATHER_END_DATE, GATHER_INTERVAL_DAILY)\n",
    "    save_backtest_data(daily, GATHER_START_DATE_2Y, GATHER_END_DATE, GATHER_INTERVAL_DAILY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_backtest_data(start_date: str, end_date: str, interval: int, save_dir: str = BACKTEST_DIR):\n",
    "    with open(os.path.join(save_dir, f\"{start_date}_{end_date}_{interval}.pickle\"), \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "weekly = load_backtest_data(GATHER_START_DATE_10Y, GATHER_END_DATE, GATHER_INTERVAL_WEEKLY)\n",
    "daily = load_backtest_data(GATHER_START_DATE_2Y, GATHER_END_DATE, GATHER_INTERVAL_DAILY)\n",
    "\n",
    "# Cut off weekly data at GATHER_START_DATE_2Y, since I originally gathered for all years\n",
    "weekly = {k: v for k, v in weekly.items() if k < GATHER_START_DATE_2Y}\n",
    "\n",
    "# Combine results into one dataset, where dataset is {date (str) : {ticker (str): data (dict)}}\n",
    "dataset = {**weekly, **daily}\n",
    "dataset = industry_encoding(dataset)\n",
    "\n",
    "dataset_df = pd.DataFrame.from_dict(\n",
    "    {(i, j): dataset[i][j] for i in dataset.keys() for j in dataset[i].keys()},\n",
    "    orient=\"index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mMultiIndex\u001b[38;5;241m.\u001b[39mfrom_tuples([(pd\u001b[38;5;241m.\u001b[39mto_datetime(date), ticker) \u001b[38;5;28;01mfor\u001b[39;00m date, ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mindex], names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39msort_index(level\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpe_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mps_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcf_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mev_ebitda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshareholder_yield\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrsi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwacc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.index = pd.MultiIndex.from_tuples([(pd.to_datetime(date), ticker) for date, ticker in df.index], names=[\"Date\", \"Ticker\"])\n",
    "df.sort_index(level=[\"Date\", \"Ticker\"], inplace=True)\n",
    "\n",
    "for column in [\"pb_ratio\", \"pe_ratio\", \"ps_ratio\", \"pcf_ratio\", \"ev_ebitda\", \"shareholder_yield\", \"rsi\", \"price\", \"wacc\"]:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Up to GATHER_START_DATE_2Y, calculate 7day price change using previous price for eack ticker + date combo\n",
    "GATHER_START_DATE_2Y_dt = pd.to_datetime(GATHER_START_DATE_2Y)\n",
    "weekly_df = df[df.index.get_level_values('Date') <= GATHER_START_DATE_2Y_dt].copy()\n",
    "weekly_df[\"7d_price_change\"] = weekly_df.groupby(\"Ticker\")[\"price\"].pct_change(1)\n",
    "daily_df = df[df.index.get_level_values('Date') >= GATHER_START_DATE_2Y_dt - pd.DateOffset(days=7)].copy()\n",
    "daily_df[\"7d_price_change\"] = daily_df.groupby(\"Ticker\")[\"price\"].pct_change(7)\n",
    "daily_df = daily_df[daily_df.index.get_level_values('Date') > GATHER_START_DATE_2Y_dt] # remove extra rows from weekly_df\n",
    "\n",
    "df = pd.concat([weekly_df, daily_df])\n",
    "df[\"7d_price_change\"].fillna(0, inplace=True) # Set to 0 for values that can't calculate change\n",
    "\n",
    "df.index.names = [\"Date\", \"Ticker\"]\n",
    "df[\"unix_time\"] = df.index.get_level_values(\"Date\").map(lambda x: x.timestamp())\n",
    "\n",
    "# Rearrange columns for better readability\n",
    "df = df[[\"unix_time\", \"price\", \"7d_price_change\", \"rsi\", \"pb_ratio\", \"pe_ratio\", \"ps_ratio\", \"pcf_ratio\", \"ev_ebitda\", \"shareholder_yield\", \"wacc\", \"industry\", \"industry_encoded\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{BACKTEST_DIR}/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "datasetDF = pd.read_csv(\"saved_data/dataset.csv\")\n",
    "embeddingDF = pd.read_csv(\"saved_data/embeddings.csv\")\n",
    "datasetDF[\"e_vector_0\"] = embeddingDF[\"0\"]\n",
    "datasetDF[\"e_vector_1\"] = embeddingDF[\"1\"]\n",
    "datasetDF.set_index([\"Date\", \"Ticker\"], inplace=True)\n",
    "datasetDF.drop(columns=[\"industry_encoded\"], inplace=True)\n",
    "datasetDF.to_csv(\"saved_data/dynamo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "priceDF = pd.DataFrame(datasetDF)\n",
    "priceDF.drop(columns=[\"unix_time\", \"7d_price_change\", \"rsi\", \"pb_ratio\", \"pe_ratio\", \"ps_ratio\", \"pcf_ratio\", \"ev_ebitda\", \"shareholder_yield\", \"wacc\", \"industry\", \"e_vector_0\", \"e_vector_1\"], inplace=True)\n",
    "priceDF.reset_index(inplace=True)\n",
    "groupedDF = priceDF.groupby(\"Ticker\").agg({\"price\": list, \"Date\": list}).reset_index()\n",
    "groupedDF.set_index(\"Ticker\", inplace=True)\n",
    "groupedDF.to_csv(\"saved_data/prices.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
